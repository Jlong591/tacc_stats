#!/opt/apps/python/2.7.1/bin/python

import argparse, glob, gzip, os, signal, string, subprocess, sys, time
signal.signal(signal.SIGPIPE, signal.SIG_DFL)

prog = os.path.basename(sys.argv[0])
job_times_cmd = "./job_times" # XXX
hostfile_dir = "/share/sge6.2/default/tacc/hostfile_logs"
archive_dir = "/scratch/projects/tacc_stats/archive"

STATS_PROGRAM = "tacc_stats" # XXX
STATS_VERSION = "1.0.1" # XXX
FILE_TIME_MAX = 86400 + 3600 # XXX
SF_SCHEMA_CHAR = '!'
SF_DEVICES_CHAR = '@'
SF_COMMENT_CHAR = '#'
SF_PROPERTY_CHAR = '$'
SF_MARK_CHAR = '%'

opt_parser = argparse.ArgumentParser(description="Collect job stats.", prog=prog)
opt_parser.add_argument("-o", "--out-dir", dest="out_dir", default=None)
opt_parser.add_argument("-v", "--verbose", action="store_true", dest="verbose")
# opt_parser.add_argument("-b", "--begin", dest="begin")
# opt_parser.add_argument("-e", "--end", dest="end")
# opt_parser.add_argument("-h", "--hostfile", dest="hostfile")
# opt_parser.add_argument("-j", "--jobid", dest="jobid")
# opt_parser.add_argument("-f", "--fqdn", "--full-hostname", action="store_true", dest="fqdn")
# opt_parser.add_argument("-m", "--marks", action="store_true", dest="marks")
# opt_parser.add_argument("-t", "--time-fmt", metavar="FMT", dest="time_fmt", default="%b %d %H:%M:%S")

# prefix time,host,jobid,type,dev
# fs, rs
# size_format

(opt, args) = opt_parser.parse_known_args()

def trace(fmt, *args):
    if opt.verbose:
        msg = fmt % args
        sys.stderr.write(prog + ": " + msg)

def error(fmt, *args):
    msg = fmt % args
    sys.stderr.write(prog + ": " + msg)
    
def fatal(fmt, *args):
    msg = fmt % args
    sys.stderr.write(prog + ": " + msg)
    sys.exit(1)

if len(args) == 0:
    fatal("must specify a jobid\n")
jobid = args[0]
opt.out_dir = opt.out_dir or "%s.%s" % (jobid, prog)

trace("args `%s'\n", args)
trace("opt `%s'\n", opt)
trace("jobid `%s'\n", jobid)

times_proc = subprocess.Popen([job_times_cmd, jobid], stdout=subprocess.PIPE)
times_out, times_err = times_proc.communicate()
job_begin, job_end = map(long, times_out.split()) # catch ValueError

trace("job_begin `%d'\n", job_begin)
trace("job_end `%d'\n", job_end)

# Find the hostfile written during the prolog.  For example:
# /share/sge6.2/default/tacc/hostfile_logs/2011/05/19/prolog_hostfile.1957000.IV32627

# TODO Try day before or after on failure.

yyyy_mm_dd = time.strftime("%Y/%m/%d", time.localtime(job_begin))
hostfile_glob = "%s/%s/prolog_hostfile.%s.*" % (hostfile_dir, yyyy_mm_dd, jobid)
for hostfile in glob.glob(hostfile_glob):
    job_hostfile = hostfile
    break
else:
    fatal("no hostfile for job `%s'\n", jobid)

trace("job_hostfile `%s'\n", job_hostfile)

os.mkdir(opt.out_dir) # EEXIST?

class StatsType:
    def __init__(self, name, spec):
        self.name = name
        self.spec = spec
        self.dev_dict = {}
        # ...
    #
    def emit_schema(self, file):
        str_spec = string.join(self.spec)
        file.write("%s%s %s\n" % (SF_SCHEMA_CHAR, self.name, str_spec))
    #
    def emit_record(self, file, dev, vals):
        str_vals = string.join(map(str, vals))
        file.write("%s %s %s\n" % (self.name, dev, str_vals))

class StatsFile:
    def __init__(self, file):
        self.file = file
        self.time = 0
        self.jobid = "0"
        self.type_dict = {}
        self.prop_dict = {}
        self.empty = True
    #
    def set_schema(self, name, spec):
        st = self.type_dict.get(name)
        if st:
            # TODO Check that old and new schemas agree.
            pass
        else:
            self.type_dict[name] = StatsType(name, spec)
    #
    def begin_group(self, time, jobid):
        self.time = time
        self.jobid = time
        if self.empty:
            self.emit_header()
        self.file.write("\n%d %s\n" % (time, jobid))
    #
    def emit_header(self):
        self.file.write("%s%s %s\n" % (SF_COMMENT_CHAR, STATS_PROGRAM, STATS_VERSION))
        for key, val in self.prop_dict.iteritems():
            self.file.write("%s%s %s\n" % (SF_PROPERTY_CHAR, key, val))
        for st in self.type_dict.itervalues():
            st.emit_schema(self.file)
        self.empty = False
    #
    def emit_mark(self, mark):
        self.file.write("%s%s\n" % (SF_MARK_CHAR, mark))
    #
    def emit_record(self, name, dev, vals):
        st = self.type_dict.get(name)
        if st:
            st.emit_record(self.file, dev, vals)
        else:
            error("stats file `%s' contains unknown type `%s'\n", self.file.name, name)

for host in open(job_hostfile, "r"):
    host = host.strip()
    if len(host) == 0:
        continue
    host_dir = os.path.join(archive_dir, host)
    trace("host `%s', host_dir `%s'\n", host, host_dir)
    host_file_info = []
    for path in os.listdir(host_dir):
        if path[0] == '.':
            continue
        file_name, file_ext = path.split('.')
        # Prune to files that might overlap with job.
        file_begin = long(file_name)
        file_end_max = file_begin + FILE_TIME_MAX
        if max(job_begin, file_begin) <= min(job_end, file_end_max):
            full_path = os.path.join(host_dir, path)
            host_file_info.append((file_begin, file_ext, full_path))
    if len(host_file_info) == 0:
        error("host `%s' has no stats files overlapping job `%s'\n", host, jobid)
        continue
    host_file_info.sort(key=lambda info: info[0])
    trace("host_file_info `%s'\n", host_file_info)
    out_path = os.path.join(opt.out_dir, host)
    out_file = StatsFile(open(out_path, "w"))
    #
    begin_mark = end_mark = False
    rec_time = 0
    rec_jobid = ""
    for info in host_file_info:
        for line in gzip.open(info[2]):
            c = line[0]
            #
            if c == SF_SCHEMA_CHAR:
                rec = line[1:].split()
                out_file.set_schema(rec[0], rec[1:])
            #
            elif c == SF_DEVICES_CHAR:
                pass # TODO
            #
            elif c == SF_COMMENT_CHAR:
                pass
            #
            elif c == SF_PROPERTY_CHAR:
                pass # TODO out_file.emit_property(line[1:))
            #
            elif c == SF_MARK_CHAR:
                mark = line[1:].strip()
                if mark == "begin %s" % jobid:
                    begin_mark = True
                elif mark == "end %s" % jobid:
                    end_mark = True
                if rec_jobid == jobid:
                    out_file.emit_mark(mark)
            #
            elif c.isdigit():
                (str_time, rec_jobid) = line.split()
                rec_time = long(str_time)
                trace("rec_jobid `%s'\n", rec_jobid)
                if rec_jobid == jobid:
                    out_file.begin_group(rec_time, rec_jobid)
            #
            elif c.isalpha():
                if rec_jobid == jobid:
                    rec = line.split()
                    out_file.emit_record(rec[0], rec[1], map(long, rec[2:]))
            #
    if not begin_mark:
        error("no begin mark found for host `%s'\n", host)
    if not end_mark:
        error("no end mark found for host `%s'\n", host)
